#!/usr/bin/env python3
"""
Local LLM Setup Script
Helps users install and configure Ollama with DeepSeek for local content generation
"""

import os
import sys
import subprocess
import requests
import time
from pathlib import Path

def check_ollama_installed():
    """Check if Ollama is installed"""
    try:
        result = subprocess.run(['ollama', '--version'], 
                              capture_output=True, text=True, timeout=10)
        return result.returncode == 0
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return False

def check_ollama_running():
    """Check if Ollama service is running"""
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        return response.status_code == 200
    except:
        return False

def start_ollama_service():
    """Start Ollama service"""
    try:
        if os.name == 'nt':  # Windows
            subprocess.Popen(['ollama', 'serve'], 
                           creationflags=subprocess.CREATE_NEW_CONSOLE)
        else:  # Linux/Mac
            subprocess.Popen(['ollama', 'serve'])
        
        print("â³ Starting Ollama service...")
        time.sleep(5)
        return check_ollama_running()
    except Exception as e:
        print(f"âŒ Error starting Ollama: {e}")
        return False

def list_available_models():
    """List available models in Ollama"""
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=10)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return [model['name'] for model in models]
        return []
    except:
        return []

def pull_model(model_name):
    """Pull a model using Ollama"""
    try:
        print(f"ğŸ“¥ Downloading {model_name}... (This may take several minutes)")
        result = subprocess.run(['ollama', 'pull', model_name], 
                              capture_output=True, text=True, timeout=1800)  # 30 min timeout
        
        if result.returncode == 0:
            print(f"âœ… Successfully downloaded {model_name}")
            return True
        else:
            print(f"âŒ Failed to download {model_name}: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print(f"âŒ Timeout downloading {model_name}")
        return False
    except Exception as e:
        print(f"âŒ Error downloading {model_name}: {e}")
        return False

def test_model(model_name):
    """Test a model with a simple prompt"""
    try:
        print(f"ğŸ§ª Testing {model_name}...")
        
        response = requests.post('http://localhost:11434/api/generate',
                               json={
                                   'model': model_name,
                                   'prompt': 'Write a brief introduction about AI marketing.',
                                   'stream': False
                               },
                               timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            content = result.get('response', '')
            if content and len(content) > 50:
                print(f"âœ… {model_name} is working correctly!")
                print(f"ğŸ“ Sample output: {content[:100]}...")
                return True
            else:
                print(f"âŒ {model_name} returned empty or short response")
                return False
        else:
            print(f"âŒ {model_name} test failed: HTTP {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Error testing {model_name}: {e}")
        return False

def main():
    """Main setup function"""
    
    print("ğŸš€ Local LLM Setup for Content Generation")
    print("=" * 50)
    
    # Step 1: Check Ollama installation
    print("\n1ï¸âƒ£ Checking Ollama installation...")
    
    if not check_ollama_installed():
        print("âŒ Ollama is not installed")
        print("\nğŸ“‹ Installation Instructions:")
        print("1. Visit: https://ollama.ai")
        print("2. Download Ollama for your operating system")
        print("3. Install and restart this script")
        return False
    
    print("âœ… Ollama is installed")
    
    # Step 2: Check if Ollama is running
    print("\n2ï¸âƒ£ Checking Ollama service...")
    
    if not check_ollama_running():
        print("âš ï¸ Ollama service is not running")
        if start_ollama_service():
            print("âœ… Ollama service started")
        else:
            print("âŒ Failed to start Ollama service")
            print("ğŸ’¡ Try running 'ollama serve' manually in another terminal")
            return False
    else:
        print("âœ… Ollama service is running")
    
    # Step 3: Check available models
    print("\n3ï¸âƒ£ Checking available models...")
    
    available_models = list_available_models()
    print(f"ğŸ“¦ Currently installed models: {len(available_models)}")
    
    for model in available_models:
        print(f"  - {model}")
    
    # Step 4: Recommend and install models
    print("\n4ï¸âƒ£ Recommended models for content generation:")
    
    recommended_models = [
        {
            "name": "deepseek-coder:6.7b",
            "description": "DeepSeek Coder - Excellent for structured content",
            "size": "~4GB"
        },
        {
            "name": "llama2:7b",
            "description": "Llama 2 - General purpose, reliable",
            "size": "~4GB"
        },
        {
            "name": "mistral:7b",
            "description": "Mistral - Fast and efficient",
            "size": "~4GB"
        }
    ]
    
    for i, model in enumerate(recommended_models, 1):
        status = "âœ… Installed" if model["name"] in available_models else "âŒ Not installed"
        print(f"{i}. {model['name']} - {model['description']} ({model['size']}) - {status}")
    
    # Step 5: Install missing models
    print("\n5ï¸âƒ£ Model installation:")
    
    missing_models = [m for m in recommended_models if m["name"] not in available_models]
    
    if not missing_models:
        print("âœ… All recommended models are already installed!")
    else:
        print(f"ğŸ“¥ {len(missing_models)} models need to be installed")
        
        install_choice = input("\nInstall missing models? (y/n): ").lower().strip()
        
        if install_choice == 'y':
            for model in missing_models:
                if pull_model(model["name"]):
                    available_models.append(model["name"])
        else:
            print("â­ï¸ Skipping model installation")
    
    # Step 6: Test models
    print("\n6ï¸âƒ£ Testing installed models...")
    
    working_models = []
    for model_name in available_models:
        if test_model(model_name):
            working_models.append(model_name)
    
    # Step 7: Summary and next steps
    print("\nğŸ‰ Setup Summary:")
    print(f"âœ… Ollama installed and running")
    print(f"ğŸ“¦ {len(available_models)} models installed")
    print(f"ğŸ§ª {len(working_models)} models tested successfully")
    
    if working_models:
        print(f"\nğŸš€ Ready to use local LLM content generation!")
        print(f"ğŸ’¡ Recommended model: {working_models[0]}")
        print(f"\nğŸ“‹ Next steps:")
        print(f"1. Run the content dashboard: python content_dashboard.py")
        print(f"2. Select 'Local LLM' as generation method")
        print(f"3. Click 'Test Local LLM' to verify connection")
        print(f"4. Start creating content with $0.00 cost!")
        
        # Test the local LLM content generator
        print(f"\nğŸ§ª Testing local LLM content generator...")
        try:
            from core.local_llm_content import MultilingualLocalLLMContentGenerator
            
            generator = MultilingualLocalLLMContentGenerator()
            generator.set_provider("deepseek" if "deepseek-coder:6.7b" in working_models else "ollama")
            
            if generator.test_connection():
                print(f"âœ… Local LLM content generator is ready!")
            else:
                print(f"âš ï¸ Local LLM content generator needs configuration")
                
        except ImportError:
            print(f"âš ï¸ local_llm_content.py not found in current directory")
        except Exception as e:
            print(f"âš ï¸ Error testing content generator: {e}")
    else:
        print(f"\nâŒ No working models found")
        print(f"ğŸ’¡ Try installing models manually: ollama pull llama2:7b")
    
    return len(working_models) > 0

if __name__ == "__main__":
    success = main()
    
    if success:
        print(f"\nğŸŠ Local LLM setup completed successfully!")
    else:
        print(f"\nâŒ Setup incomplete. Please check the instructions above.")
    
    input(f"\nPress Enter to exit...") 