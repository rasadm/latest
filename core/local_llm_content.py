#!/usr/bin/env python3
"""
Enhanced Multilingual Local LLM Content Generator
Supports English, Farsi (Persian), and Spanish content generation with cultural awareness
"""

import os
import json
import time
import random
import requests
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from core.settings_manager import SettingsManager

class MultilingualLocalLLMContentGenerator:
    def __init__(self):
        self.settings_manager = SettingsManager()
        
        # LLM Provider configurations
        self.providers = {
            "deepseek": {
                "api_url": "http://localhost:11434/api/generate",
                "model": "deepseek-coder:6.7b",
                "cost_per_token": 0.0,
                "max_tokens": 32768  # DeepSeek has large context window
            },
            "deepseek-32b": {
                "api_url": "http://localhost:11434/api/generate", 
                "model": "deepseek-coder:32b",
                "cost_per_token": 0.0,
                "max_tokens": 32768  # Even larger context for 32B model
            },
            "deepseek-14b": {
                "api_url": "http://localhost:11434/api/generate",
                "model": "deepseek-coder:14b", 
                "cost_per_token": 0.0,
                "max_tokens": 32768  # Large context window
            },
            "llama3.3:latest": {
                "api_url": "http://localhost:11434/api/generate",
                "model": "llama3.3:latest",
                "cost_per_token": 0.0,
                "max_tokens": 16384  # Llama 3.3 context window
            },
            "ollama": {
                "api_url": "http://localhost:11434/api/generate",
                "model": "llama2:7b",
                "cost_per_token": 0.0,
                "max_tokens": 8192  # Older model, more conservative
            },
            "llamacpp": {
                "api_url": "http://localhost:8080/completion",
                "model": "local",
                "cost_per_token": 0.0,
                "max_tokens": 16384  # Configurable based on loaded model
            },
            "claude": {
                "api_url": "https://api.anthropic.com/v1/messages",
                "model": "claude-3-5-sonnet-20241022",
                "cost_per_token": 0.003,
                "max_tokens": 8192,
                "api_key_required": True
            }
        }
        
        self.current_provider = "deepseek"  # Default to DeepSeek
        
        # Load multilingual content topics from config
        self.multilingual_topics = self._load_multilingual_topics_config()

    def _load_multilingual_topics_config(self):
        """Load multilingual topics configuration from file with fallback defaults"""
        config_file = "multilingual_topics_config.json"
        
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"Warning: Could not load multilingual topics config: {e}")
        
        # Return default configuration as fallback
        return self._get_default_multilingual_topics()
    
    def _get_default_multilingual_topics(self):
        """Get default multilingual topics configuration"""
        return {
            "english": [
                "AI Marketing Automation Trends",
                "Machine Learning in Customer Experience", 
                "Predictive Analytics for Marketing",
                "Conversational AI and Chatbots",
                "AI-Powered Content Personalization",
                "Marketing Attribution with AI",
                "Voice Search Optimization",
                "AI in Social Media Marketing",
                "Programmatic Advertising Evolution",
                "Customer Data Platform Innovation"
            ],
            "farsi": [
                "Ø±ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ† Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ",
                "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¯Ø± ØªØ¬Ø±Ø¨Ù‡ Ù…Ø´ØªØ±ÛŒ",
                "ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ",
                "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒØ§ÛŒ Ùˆ Ú†Øªâ€ŒØ¨Ø§Øªâ€ŒÙ‡Ø§",
                "Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ",
                "Ø§Ù†ØªØ³Ø§Ø¨ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ",
                "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ ØµÙˆØªÛŒ",
                "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ",
                "ØªÚ©Ø§Ù…Ù„ ØªØ¨Ù„ÛŒØºØ§Øª Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ§ÛŒ",
                "Ù†ÙˆØ¢ÙˆØ±ÛŒ Ø¯Ø± Ù¾Ù„ØªÙØ±Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´ØªØ±ÛŒ"
            ],
            "spanish": [
                "Tendencias de AutomatizaciÃ³n de Marketing con IA",
                "Aprendizaje AutomÃ¡tico en la Experiencia del Cliente",
                "AnÃ¡lisis Predictivo para Marketing",
                "IA Conversacional y Chatbots",
                "PersonalizaciÃ³n de Contenido con IA",
                "AtribuciÃ³n de Marketing con IA",
                "OptimizaciÃ³n de BÃºsqueda por Voz",
                "IA en Marketing de Redes Sociales",
                "EvoluciÃ³n de la Publicidad ProgramÃ¡tica",
                "InnovaciÃ³n en Plataformas de Datos de Clientes"
            ]
        }

    def set_provider(self, provider: str, custom_config: Dict = None):
        """Set the local LLM provider"""
        if provider in self.providers:
            self.current_provider = provider
        elif custom_config:
            self.providers[provider] = custom_config
            self.current_provider = provider
        else:
            raise ValueError(f"Unknown provider: {provider}")

    def test_connection(self, provider: str = None) -> bool:
        """Test connection to local LLM"""
        test_provider = provider or self.current_provider
        config = self.providers[test_provider]
        
        try:
            if test_provider == "claude":
                # Claude API format
                import os
                api_key = os.getenv('ANTHROPIC_API_KEY')
                if not api_key:
                    print("âŒ ANTHROPIC_API_KEY environment variable not set")
                    return False
                
                headers = {
                    "x-api-key": api_key,
                    "content-type": "application/json",
                    "anthropic-version": "2023-06-01"
                }
                
                response = requests.post(
                    config["api_url"],
                    headers=headers,
                    json={
                        "model": config["model"],
                        "max_tokens": 10,
                        "messages": [{"role": "user", "content": "Hello"}]
                    },
                    timeout=30
                )
                return response.status_code == 200
                
            elif test_provider in ["deepseek", "deepseek-32b", "deepseek-14b", "llama3.3:latest", "ollama"]:
                # Ollama API format
                response = requests.post(
                    config["api_url"],
                    json={
                        "model": config["model"],
                        "prompt": "Hello",
                        "stream": False,
                        "options": {"max_tokens": 10}
                    },
                    timeout=30
                )
                return response.status_code == 200
            else:
                # llama.cpp format
                response = requests.post(
                    config["api_url"],
                    json={
                        "prompt": "Hello",
                        "n_predict": 10
                    },
                    timeout=30
                )
                return response.status_code == 200
                
        except Exception as e:
            print(f"Connection test failed: {e}")
            return False

    def generate_content_with_local_llm(self, topic: str, content_type: str = "blog_post", language: str = "english") -> str:
        """Generate multilingual content using local LLM"""
        
        prompt = self.create_multilingual_content_prompt(topic, content_type, language)
        config = self.providers[self.current_provider]
        
        try:
            if self.current_provider in ["deepseek", "deepseek-32b", "deepseek-14b", "llama3.3:latest", "ollama"]:
                # Ollama API format
                response = requests.post(
                    config["api_url"],
                    json={
                        "model": config["model"],
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "top_p": 0.9,
                            "max_tokens": config["max_tokens"]
                        }
                    },
                    timeout=300  # RTX 5090 generating 4K+ word content needs more time
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", "")
                
            else:
                # llama.cpp format
                response = requests.post(
                    config["api_url"],
                    json={
                        "prompt": prompt,
                        "n_predict": config["max_tokens"],
                        "temperature": 0.7,
                        "top_p": 0.9
                    },
                    timeout=300  # Extended timeout for comprehensive content
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get("content", "")
            
            print(f"âŒ LLM API error: {response.status_code}")
            return None
            
        except Exception as e:
            print(f"âŒ Error generating content: {e}")
            return None
    
    def create_multilingual_content_prompt(self, topic: str, content_type: str, language: str) -> str:
        """Create language-specific optimized prompt for local LLM"""
        
        lang_config = self.settings_manager.get_language_config(language)
        if not lang_config:
            language = "english"  # Fallback
            lang_config = self.settings_manager.get_language_config(language)
        
        cultural_rules = lang_config.cultural_rules
        editorial_rules = lang_config.editorial_rules
        content_templates = lang_config.content_templates
        
        if content_type == "blog_post":
            if language == "english":
                return self._create_english_blog_prompt(topic, cultural_rules, editorial_rules)
            elif language == "farsi":
                return self._create_farsi_blog_prompt(topic, cultural_rules, editorial_rules, content_templates)
            elif language == "spanish":
                return self._create_spanish_blog_prompt(topic, cultural_rules, editorial_rules, content_templates)
        
        elif content_type == "section":
            if language == "english":
                return self._create_english_section_prompt(topic)
            elif language == "farsi":
                return self._create_farsi_section_prompt(topic, content_templates)
            elif language == "spanish":
                return self._create_spanish_section_prompt(topic, content_templates)
        
        else:
            return f"Write professional marketing content about: {topic}"
    
    def _create_english_blog_prompt(self, topic: str, cultural_rules: Dict, editorial_rules: Dict) -> str:
        """Create English blog post prompt"""
        return f"""Write a comprehensive, in-depth 3500-4000 word blog post about "{topic}" for modern marketers. With your expanded context window, create the most detailed and valuable content possible.

Structure:
1. Executive Summary (300 words)
2. Introduction & Market Context (400 words)
3. Current State Analysis (600 words)
4. Key Trends and Technologies (800 words)
5. Implementation Strategies & Best Practices (700 words)
6. Case Studies & Real-World Examples (500 words)
7. Challenges & Solutions (400 words)
8. Future Outlook & Predictions (500 words)
9. Actionable Recommendations (300 words)
10. Conclusion (200 words)

Requirements:
- Professional, authoritative tone with expert-level insights
- Include specific statistics, data points, and research findings
- Provide detailed, actionable strategies with step-by-step guidance
- Focus on practical implementation with real-world examples
- Target audience: Marketing professionals, business leaders, and decision-makers
- Include relevant case studies from major companies
- Add specific tools, platforms, and technologies mentioned by name
- Include budget considerations and ROI expectations
- Provide timeline estimates for implementation
- Address common challenges and their solutions
- Use markdown formatting with proper headers, lists, and emphasis
- Include relevant quotes from industry experts (create realistic ones)
- Add specific metrics and KPIs to track success

Topic: {topic}

Create the most comprehensive, valuable, and actionable blog post possible. Use your full context window to provide maximum value:"""
    
    def _create_farsi_blog_prompt(self, topic: str, cultural_rules: Dict, editorial_rules: Dict, content_templates: Dict) -> str:
        """Create Farsi (Persian) blog post prompt with cultural considerations"""
        return f"""ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡ Ø¬Ø§Ù…Ø¹ Ùˆ Ø¹Ù…ÛŒÙ‚ Û³Û°Û°Û°-Û³ÛµÛ°Û° Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ "{topic}" Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨Ø§Ù† Ù…Ø¯Ø±Ù† Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯. Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯ÛŒØ±ÛŒ ÙØ±Ù‡Ù†Ú¯ Ùˆ Ø§Ø±Ø²Ø´â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒØŒ Ù…Ø­ØªÙˆØ§ÛŒÛŒ Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯.

Ø³Ø§Ø®ØªØ§Ø±:
Û±. Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§ÛŒÛŒ (Û²ÛµÛ° Ú©Ù„Ù…Ù‡)
Û². Ù…Ù‚Ø¯Ù…Ù‡ Ùˆ Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ø§Ø²Ø§Ø± (Û³ÛµÛ° Ú©Ù„Ù…Ù‡)
Û³. ØªØ­Ù„ÛŒÙ„ ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ (ÛµÛ°Û° Ú©Ù„Ù…Ù‡)
Û´. Ø±ÙˆÙ†Ø¯Ù‡Ø§ Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ (Û·Û°Û° Ú©Ù„Ù…Ù‡)
Ûµ. Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø¨Ù‡ØªØ±ÛŒÙ† Ø´ÛŒÙˆÙ‡â€ŒÙ‡Ø§ (Û¶Û°Û° Ú©Ù„Ù…Ù‡)
Û¶. Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ù…ÙˆØ±Ø¯ÛŒ Ùˆ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ (Û´ÛµÛ° Ú©Ù„Ù…Ù‡)
Û·. Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§ Ùˆ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ (Û³ÛµÛ° Ú©Ù„Ù…Ù‡)
Û¸. Ú†Ø´Ù…â€ŒØ§Ù†Ø¯Ø§Ø² Ø¢ÛŒÙ†Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ (Û´ÛµÛ° Ú©Ù„Ù…Ù‡)
Û¹. ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ (Û²ÛµÛ° Ú©Ù„Ù…Ù‡)
Û±Û°. Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ (Û±ÛµÛ° Ú©Ù„Ù…Ù‡)

Ø§Ù„Ø²Ø§Ù…Ø§Øª ÙØ±Ù‡Ù†Ú¯ÛŒ Ùˆ ÙˆÛŒØ±Ø§Ø³ØªØ§Ø±ÛŒ:
- Ù„Ø­Ù† Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡ Ùˆ Ø±Ø³Ù…ÛŒ Ø¨Ø§ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø¤Ø¯Ø¨Ø§Ù†Ù‡ Ùˆ Ø§Ø­ØªØ±Ø§Ù…â€ŒØ¢Ù…ÛŒØ²
- Ø¯Ø± Ù†Ø¸Ø± Ú¯ÛŒØ±ÛŒ Ø§Ø±Ø²Ø´â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ Ùˆ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
- Ø§Ø±Ø§Ø¦Ù‡ Ø¢Ù…Ø§Ø± Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯
- Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø§ Ù…Ø±Ø§Ø­Ù„ Ù…Ø´Ø®Øµ
- Ù…Ø®Ø§Ø·Ø¨ Ù‡Ø¯Ù: Ù…ØªØ®ØµØµØ§Ù† Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒØŒ Ø±Ù‡Ø¨Ø±Ø§Ù† Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±ØŒ Ùˆ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±Ù†Ø¯Ú¯Ø§Ù†
- Ø´Ø§Ù…Ù„ Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ù…ÙˆØ±Ø¯ÛŒ Ø§Ø² Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
- Ø°Ú©Ø± Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ØŒ Ù¾Ù„ØªÙØ±Ù…â€ŒÙ‡Ø§ Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®Øµ
- Ø¯Ø± Ù†Ø¸Ø± Ú¯ÛŒØ±ÛŒ Ø¨ÙˆØ¯Ø¬Ù‡ Ùˆ Ø§Ù†ØªØ¸Ø§Ø±Ø§Øª Ø¨Ø§Ø²Ú¯Ø´Øª Ø³Ø±Ù…Ø§ÛŒÙ‡
- Ø§Ø±Ø§Ø¦Ù‡ Ø¨Ø±Ø¢ÙˆØ±Ø¯ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
- Ù¾Ø±Ø¯Ø§Ø®ØªÙ† Ø¨Ù‡ Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬ Ùˆ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‚Ø§Ù„Ø¨â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø§Ø±Ú©â€ŒØ¯Ø§ÙˆÙ† Ø¨Ø§ Ø³Ø±ØªÛŒØªØ±Ù‡Ø§ØŒ ÙÙ‡Ø±Ø³Øªâ€ŒÙ‡Ø§ Ùˆ ØªØ£Ú©ÛŒØ¯Ø§Øª Ù…Ù†Ø§Ø³Ø¨
- Ø´Ø§Ù…Ù„ Ù†Ù‚Ù„â€ŒÙ‚ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø² Ú©Ø§Ø±Ø´Ù†Ø§Ø³Ø§Ù† ØµÙ†Ø¹Øª
- Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ùˆ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯

Ù…ÙˆØ¶ÙˆØ¹: {topic}

{content_templates.get('respectful_address', 'Ø¬Ù†Ø§Ø¨ Ø¢Ù‚Ø§ÛŒ / Ø³Ø±Ú©Ø§Ø± Ø®Ø§Ù†Ù…')} Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡ Ù…Ø­ØªØ±Ù…ØŒ

Ø¬Ø§Ù…Ø¹â€ŒØªØ±ÛŒÙ†ØŒ Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ØªØ±ÛŒÙ† Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒâ€ŒØªØ±ÛŒÙ† Ù…Ù‚Ø§Ù„Ù‡ Ù…Ù…Ú©Ù† Ø±Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯. Ø§Ø² ØªÙ…Ø§Ù… Ø¸Ø±ÙÛŒØª Ø®ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø­Ø¯Ø§Ú©Ø«Ø± Ø§Ø±Ø²Ø´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:"""
    
    def _create_spanish_blog_prompt(self, topic: str, cultural_rules: Dict, editorial_rules: Dict, content_templates: Dict) -> str:
        """Create Spanish blog post prompt with cultural considerations"""
        return f"""Escriba un artÃ­culo integral y profundo de 3200-3700 palabras sobre "{topic}" para profesionales de marketing modernos. Considerando la cultura hispana y los valores familiares, cree contenido valioso y prÃ¡ctico.

Estructura:
1. Resumen Ejecutivo (280 palabras)
2. IntroducciÃ³n y Contexto del Mercado (380 palabras)
3. AnÃ¡lisis del Estado Actual (550 palabras)
4. Tendencias y TecnologÃ­as Clave (750 palabras)
5. Estrategias de ImplementaciÃ³n y Mejores PrÃ¡cticas (650 palabras)
6. Estudios de Caso y Ejemplos del Mundo Real (480 palabras)
7. DesafÃ­os y Soluciones (380 palabras)
8. Perspectivas Futuras y Predicciones (480 palabras)
9. Recomendaciones PrÃ¡cticas (280 palabras)
10. ConclusiÃ³n (180 palabras)

Requisitos Culturales y Editoriales:
- Tono cÃ¡lido y profesional con perspectivas expertas
- Enfoque en relaciones y valores familiares cuando sea apropiado
- ConsideraciÃ³n de la diversidad regional hispana
- Incluir estadÃ­sticas especÃ­ficas, datos y hallazgos de investigaciÃ³n
- Proporcionar estrategias detalladas y prÃ¡cticas con orientaciÃ³n paso a paso
- Enfoque en implementaciÃ³n prÃ¡ctica con ejemplos del mundo real
- Audiencia objetivo: Profesionales de marketing, lÃ­deres empresariales y tomadores de decisiones
- Incluir estudios de caso relevantes de empresas importantes
- Agregar herramientas, plataformas y tecnologÃ­as especÃ­ficas mencionadas por nombre
- Incluir consideraciones presupuestarias y expectativas de ROI
- Proporcionar estimaciones de tiempo para la implementaciÃ³n
- Abordar desafÃ­os comunes y sus soluciones
- Usar formato markdown con encabezados, listas y Ã©nfasis apropiados
- Incluir citas relevantes de expertos de la industria (crear realistas)
- Agregar mÃ©tricas especÃ­ficas y KPIs para rastrear el Ã©xito

Tema: {topic}

{content_templates.get('formal_address', 'Estimado/a')} profesional,

Cree el artÃ­culo mÃ¡s completo, valioso y prÃ¡ctico posible. Use toda su capacidad para proporcionar el mÃ¡ximo valor:"""
    
    def _create_english_section_prompt(self, topic: str) -> str:
        """Create English section prompt"""
        return f"""Write a comprehensive 600-800 word section about "{topic}" for a marketing blog post. With increased context capacity, provide maximum detail and value.

Requirements:
- Professional, expert-level tone
- Include specific examples with company names and results
- Provide detailed, actionable insights with step-by-step guidance
- Use relevant data, statistics, and research findings
- Focus on practical implementation value for marketers
- Include specific tools, platforms, and technologies
- Add budget considerations and timeline estimates
- Address potential challenges and solutions
- Use proper markdown formatting with subheadings and lists
- Include relevant metrics and KPIs

Create the most detailed and valuable section possible:"""
    
    def _create_farsi_section_prompt(self, topic: str, content_templates: Dict) -> str:
        """Create Farsi section prompt"""
        return f"""Ø¨Ø®Ø´ Ø¬Ø§Ù…Ø¹ ÛµÛ°Û°-Û·Û°Û° Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ "{topic}" Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ù„Ù‡ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯. Ø¨Ø§ Ø¸Ø±ÙÛŒØª Ø§ÙØ²Ø§ÛŒØ´â€ŒÛŒØ§ÙØªÙ‡ØŒ Ø­Ø¯Ø§Ú©Ø«Ø± Ø¬Ø²Ø¦ÛŒØ§Øª Ùˆ Ø§Ø±Ø²Ø´ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.

Ø§Ù„Ø²Ø§Ù…Ø§Øª:
- Ù„Ø­Ù† Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ùˆ ØªØ®ØµØµÛŒ
- Ø´Ø§Ù…Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ø¨Ø§ Ù†Ø§Ù… Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ Ùˆ Ù†ØªØ§ÛŒØ¬
- Ø§Ø±Ø§Ø¦Ù‡ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø¹Ù…Ù„ÛŒ Ø¨Ø§ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ú¯Ø§Ù…â€ŒØ¨Ù‡â€ŒÚ¯Ø§Ù…
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ø¢Ù…Ø§Ø± Ùˆ ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ Ù…Ø±ØªØ¨Ø·
- ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ø§Ø±Ø²Ø´ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨Ø§Ù†
- Ø´Ø§Ù…Ù„ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ØŒ Ù¾Ù„ØªÙØ±Ù…â€ŒÙ‡Ø§ Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®Øµ
- Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¨ÙˆØ¯Ø¬Ù‡ Ùˆ Ø¨Ø±Ø¢ÙˆØ±Ø¯ Ø²Ù…Ø§Ù†ÛŒ
- Ù¾Ø±Ø¯Ø§Ø®ØªÙ† Ø¨Ù‡ Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ùˆ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‚Ø§Ù„Ø¨â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø§Ø±Ú©â€ŒØ¯Ø§ÙˆÙ† Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø§ Ø²ÛŒØ±Ø¹Ù†ÙˆØ§Ù†â€ŒÙ‡Ø§ Ùˆ ÙÙ‡Ø±Ø³Øªâ€ŒÙ‡Ø§
- Ø´Ø§Ù…Ù„ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ùˆ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø±ØªØ¨Ø·

Ù…ÙØµÙ„â€ŒØªØ±ÛŒÙ† Ùˆ Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ØªØ±ÛŒÙ† Ø¨Ø®Ø´ Ù…Ù…Ú©Ù† Ø±Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯:"""
    
    def _create_spanish_section_prompt(self, topic: str, content_templates: Dict) -> str:
        """Create Spanish section prompt"""
        return f"""Escriba una secciÃ³n integral de 550-750 palabras sobre "{topic}" para un artÃ­culo de marketing. Con capacidad aumentada, proporcione el mÃ¡ximo detalle y valor.

Requisitos:
- Tono profesional y experto
- Incluir ejemplos especÃ­ficos con nombres de empresas y resultados
- Proporcionar perspectivas detalladas y prÃ¡cticas con orientaciÃ³n paso a paso
- Usar datos relevantes, estadÃ­sticas y hallazgos de investigaciÃ³n
- Enfoque en valor de implementaciÃ³n prÃ¡ctica para profesionales de marketing
- Incluir herramientas, plataformas y tecnologÃ­as especÃ­ficas
- Agregar consideraciones presupuestarias y estimaciones de tiempo
- Abordar desafÃ­os potenciales y soluciones
- Usar formato markdown apropiado con subtÃ­tulos y listas
- Incluir mÃ©tricas y KPIs relevantes

Cree la secciÃ³n mÃ¡s detallada y valiosa posible:"""

    def generate_multilingual_blog_post(self, topic: str, language: str = "english") -> Dict:
        """Generate complete multilingual blog post using local LLM"""
        
        print(f"ğŸ¤– Generating {language} content with {self.current_provider.upper()}...")
        
        # Generate main content
        content = self.generate_content_with_local_llm(topic, "blog_post", language)
        
        if not content:
            print(f"âŒ Failed to generate {language} content with local LLM")
            return None
        
        # Get language configuration for metadata
        lang_config = self.settings_manager.get_language_config(language)
        content_templates = lang_config.content_templates if lang_config else {}
        
        # Create language-specific metadata
        if language == "english":
            title = f"{topic}: {datetime.now().year} Trends and Implementation Guide for Modern Marketers"
            description = f"Comprehensive guide to {topic.lower()} implementation, trends, and best practices for marketing professionals in {datetime.now().year}."
            tags = [
                topic.lower().replace(" ", "-"),
                "ai-marketing",
                "marketing-automation", 
                "digital-transformation",
                f"{datetime.now().year}-trends",
                "local-llm-generated"
            ]
            categories = ["AI Marketing", "Marketing Technology", "Digital Innovation"]
            
        elif language == "farsi":
            title = f"{topic}: Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Ø±ÙˆÙ†Ø¯Ù‡Ø§ Ùˆ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨Ø§Ù† Ù…Ø¯Ø±Ù† {datetime.now().year}"
            description = f"Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ {topic}ØŒ Ø±ÙˆÙ†Ø¯Ù‡Ø§ Ùˆ Ø¨Ù‡ØªØ±ÛŒÙ† Ø´ÛŒÙˆÙ‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…ØªØ®ØµØµØ§Ù† Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ø¯Ø± Ø³Ø§Ù„ {datetime.now().year}."
            tags = [
                topic.lower().replace(" ", "-"),
                "Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ-Ù‡ÙˆØ´-Ù…ØµÙ†ÙˆØ¹ÛŒ",
                "Ø§ØªÙˆÙ…Ø§Ø³ÛŒÙˆÙ†-Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ",
                "ØªØ­ÙˆÙ„-Ø¯ÛŒØ¬ÛŒØªØ§Ù„",
                f"Ø±ÙˆÙ†Ø¯Ù‡Ø§ÛŒ-{datetime.now().year}",
                "ØªÙˆÙ„ÛŒØ¯-Ù…Ø­Ù„ÛŒ-llm"
            ]
            categories = ["Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ", "ÙÙ†Ø§ÙˆØ±ÛŒ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ", "Ù†ÙˆØ¢ÙˆØ±ÛŒ Ø¯ÛŒØ¬ÛŒØªØ§Ù„"]
            
        elif language == "spanish":
            title = f"{topic}: GuÃ­a Integral de Tendencias e ImplementaciÃ³n para Profesionales de Marketing {datetime.now().year}"
            description = f"GuÃ­a completa de implementaciÃ³n de {topic.lower()}, tendencias y mejores prÃ¡cticas para profesionales de marketing en {datetime.now().year}."
            tags = [
                topic.lower().replace(" ", "-"),
                "marketing-ia",
                "automatizaciÃ³n-marketing",
                "transformaciÃ³n-digital",
                f"tendencias-{datetime.now().year}",
                "generado-llm-local"
            ]
            categories = ["Marketing IA", "TecnologÃ­a de Marketing", "InnovaciÃ³n Digital"]
        
        # Create filename
        filename = title.lower().replace(" ", "-").replace(":", "").replace(",", "")
        filename = "".join(c for c in filename if c.isalnum() or c in "-_")
        filename = f"{filename}-{datetime.now().strftime('%Y-%m-%d')}.md"
        
        # Create YAML front matter
        front_matter = {
            "title": title,
            "description": description,
            "categories": categories,
            "tags": tags,
            "date": datetime.now().strftime("%Y-%m-%d"),
            "author": "RasaDM AI Research Team",
            "reading_time": f"{random.randint(8, 15)} minutes",
            "seo_keywords": ", ".join(tags[:5]),
            "generated_by": f"local_llm_{self.current_provider}",
            "cost": "$0.00 (Local Generation)",
            "model": self.providers[self.current_provider]["model"],
            "language": language,
            "cultural_context": lang_config.cultural_rules.get("cultural_context", "general") if lang_config else "general"
        }
        
        return {
            "title": title,
            "content": content,
            "metadata": front_matter,
            "filename": filename,
            "topic": topic,
            "provider": self.current_provider,
            "language": language
        }

def setup_deepseek_ollama():
    """Setup instructions for DeepSeek with Ollama"""
    
    instructions = """
ğŸš€ Setup DeepSeek with Ollama (Recommended)

1. Install Ollama:
   - Download from: https://ollama.ai
   - Run: ollama serve

2. Install DeepSeek model:
   - Run: ollama pull deepseek-coder:6.7b
   - Or: ollama pull deepseek-llm:7b

3. Test installation:
   - Run: ollama run deepseek-coder:6.7b "Hello"

4. Alternative models:
   - ollama pull llama2:7b (General purpose)
   - ollama pull codellama:7b (Code-focused)
   - ollama pull mistral:7b (Fast and efficient)

ğŸ’° Cost Benefits:
- $0.00 per post (vs $0.01-0.05 with cloud APIs)
- Complete privacy (no data sent to external servers)
- Unlimited usage (no rate limits)
- Works offline

ğŸ”§ Hardware Requirements:
- Minimum: 8GB RAM, 4GB VRAM
- Recommended: 16GB RAM, 8GB VRAM
- Storage: 4-8GB per model
"""
    
    print(instructions)
    return instructions

def main():
    """Test local LLM content generation"""
    
    generator = MultilingualLocalLLMContentGenerator()
    
    print("ğŸ¤– Local LLM Content Generator")
    print("=" * 40)
    
    # Test connection
    if generator.test_connection():
        print(f"âœ… Connected to {generator.current_provider}")
        
        # Generate test content
        topic = random.choice(generator.multilingual_topics["english"])
        post_data = generator.generate_multilingual_blog_post(topic)
        
        if post_data:
            file_path = generator.save_local_llm_post(post_data)
            print(f"âœ… Generated post saved to: {file_path}")
            print(f"ğŸ“ Title: {post_data['title']}")
            print(f"ğŸ¤– Provider: {post_data['provider']}")
            print(f"ğŸ’° Cost: $0.00 (Local Generation)")
        else:
            print("âŒ Failed to generate content")
    else:
        print(f"âŒ Cannot connect to {generator.current_provider}")
        print("\n" + setup_deepseek_ollama())

if __name__ == "__main__":
    main() 